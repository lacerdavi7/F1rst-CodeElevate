# ğŸš–ğŸ“Š F1RST CodeElevate â€“ Spark, Kafka e Postgres

Este repositÃ³rio contÃ©m dois assignments que exploram pipelines de dados em um ambiente de **Big Data**, utilizando **Apache Spark**, **Apache Kafka** e **Postgres**.  
O objetivo Ã© praticar **processamento batch e streaming**, organizaÃ§Ã£o de dados em camadas e construÃ§Ã£o de soluÃ§Ãµes resilientes e escalÃ¡veis.  

---

## âš™ï¸ Arquitetura da SoluÃ§Ã£o

A arquitetura Ã© composta por:

- **Producer (Kafka)** â†’ gera eventos em tempo real (dados IoT falsos).
- **Kafka Broker** â†’ atua como mensageria para transmitir eventos entre produtores e consumidores.
- **Spark** â†’ consome dados em batch e streaming, processa e transforma.
- **Postgres** â†’ armazena os resultados finais para anÃ¡lise posterior.

### ğŸ”¹ Diagrama Geral
```mermaid
flowchart LR
    subgraph PRODUCER["Producer (Python + Faker)"]
        A["IoT Sensor Data Generator"]
    end

    subgraph KAFKA["Apache Kafka"]
        B["Topic: iot_sensor"]
    end

    subgraph SPARK["Apache Spark"]
        C1["Batch Job"]
        C2["Streaming Job"]
    end

    subgraph POSTGRES["Postgres Database"]
        D["Tables (info_corridas_do_dia, medicao_sensores)"]
    end

    A --> B
    B --> C1
    B --> C2
    C1 --> D
    C2 --> D
```

---

## ğŸ“Œ Assignment 1 â€“ DiÃ¡rio de Bordo (Batch Processing)

Temos como entrada o dataset **`data/info_transportes.csv`**, com informaÃ§Ãµes de corridas realizadas via aplicativo de transporte privado.  

**Colunas originais:**
- `DATA_INICIO` â†’ formato `mm-dd-yyyy HH`
- `DATA_FIM`  
- `CATEGORIA` (NegÃ³cio, Pessoal, etc.)
- `LOCAL_INICIO`
- `LOCAL_FIM`
- `PROPOSITO` (ReuniÃ£o, Lazer, etc.)
- `DISTANCIA`

### âœ… Objetivo
Gerar uma nova tabela: **`info_corridas_do_dia`**, agrupada por **data de inÃ­cio (yyyy-MM-dd)**.  
No processamento tratamos os campos DATA_INICIO, CATEGORIA e PROPOSITO (Padronizando a entrada para que nÃ£o haja perda de informaÃ§Ã£o/classificaÃ§Ã£o no processamento)

**Colunas da tabela final:**
| Coluna | DescriÃ§Ã£o |
|--------|-----------|
| `DT_REFE` | Data de referÃªncia |
| `QT_CORR` | Quantidade de corridas |
| `QT_CORR_NEG` | Corridas categoria "NegÃ³cio" |
| `QT_CORR_PESS` | Corridas categoria "Pessoal" |
| `VL_MAX_DIST` | Maior distÃ¢ncia |
| `VL_MIN_DIST` | Menor distÃ¢ncia |
| `VL_AVG_DIST` | DistÃ¢ncia mÃ©dia |
| `QT_CORR_REUNI` | Corridas com propÃ³sito "ReuniÃ£o" |
| `QT_CORR_NAO_REUNI` | Corridas com propÃ³sito diferente de "ReuniÃ£o" |

---

## ğŸ“Œ Assignment 2 â€“ Monitoramento de Sensores IoT (Streaming)

Aqui construÃ­mos um sistema que simula sensores IoT e processa os dados em **tempo real**.  

### âœ… Fluxo do sistema
1. **Producer** â†’ gera dados falsos de sensores IoT (temperatura, umidade, localizaÃ§Ã£o, etc.) com a biblioteca `Faker`.  
2. **Kafka** â†’ recebe os dados no tÃ³pico `iot_sensor`.  
3. **Consumer (Spark Streaming)** â†’ consome os dados em tempo real e replica para o postgres a cada 20 segundos.  
4. **Postgres** â†’ armazena os dados processados para consultas e dashboards.  

### ğŸ”¹ Diagrama do Streaming
```mermaid
sequenceDiagram
    participant P as Producer (IoT Fake Data)
    participant K as Kafka Broker
    participant S as Spark Streaming
    participant DB as Postgres

    P->>K: Envia evento (iot_sensor)
    K->>S: Disponibiliza evento no tÃ³pico
    S->>S: Processa 
    S->>DB: Salva no banco de dados
```

---

## ğŸ› ï¸ Como Rodar o Projeto

### ğŸ”¹ PrÃ©-requisitos
- Docker e Docker Compose instalados
- Makefile disponÃ­vel no ambiente (`make`)

### ğŸ”¹ Comandos principais
- **Criar os serviÃ§os**:
  ```bash
  make build
  ```
- **Subir os serviÃ§os**:
  ```bash
  make run
  ```
- **Criar topico**:
  ```bash
  make create-topic
  ```
- **Executar Spark-Streaming**:
  ```bash
  make stream-iot
  ```
- **Executar Spark-batch**:
  ```bash
  make batch
  ```
- **Rodar o Producer (O processo estÃ¡ configurado para gerar dados durante 5 minutos)**:
  ```bash
  make iot-sensor
  ```
- **Acessar Postgres**:
  ```bash
  make postgres
  ```
- **Rodar os testes unitarios das funÃ§Ãµes PySpark**:
  ```bash
  make unit-test
  ```
### ğŸ”¹ Consultar tabelas no Postgres
- **Tabela Streaming**:
  ```sql
  select * from medicao_sensores limit 10;
  select count(*) from medicao_sensores;
  ```
- **Tabela Batch**:
  ```sql
  sselect * from info_corridas_do_dia limit 10;
  select count(*) from info_corridas_do_dia;
  ```

---

## ğŸ“‚ Estrutura do RepositÃ³rio
```
F1RST-CODEELEVATE/
â”œâ”€ docker-compose.yaml    # OrquestraÃ§Ã£o de serviÃ§os (Spark, Kafka, Postgres, Producer e Pyspark-tests)
â”œâ”€ Makefile               # Atalhos de execuÃ§Ã£o
â”œâ”€ data/                  # Dataset base (info_transportes.csv)
â”œâ”€ producer/              # CÃ³digo do Producer (Kafka + IoT fake data)
â”‚   â”œâ”€ Dockerfile
â”‚   â””â”€ producer.py
â”œâ”€ src/                   # CÃ³digo principal
â”‚   â”œâ”€ batch_job.py       # Pipeline batch (Assignment 1)
â”‚   â”œâ”€ stream_iot_sensor.py # Pipeline streaming (Assignment 2)
â”‚   â”œâ”€ functions.py       # FunÃ§Ãµes auxiliares
â”‚   â””â”€ tests/             # Testes PySpark
â”‚       â””â”€ test_funtions.py

```

---

## ğŸ¯ ConclusÃ£o

Este projeto cobre **duas frentes fundamentais em Big Data**:
- **Processamento Batch (Spark + CSV â†’ Postgres)**  
- **Processamento Streaming (Kafka + IoT â†’ Spark â†’ Postgres)**  

A arquitetura foi projetada para ser:
- EscalÃ¡vel  
- Resiliente a falhas  
- Focada em qualidade e integridade dos dados  

---

## âš ï¸ Alerta

Em um cenÃ¡rio real, o ideal no processamento em batch Ã© ingerir os dados de preferÃªncia no formato parquet do DataLake(camanda bronze), fazer os tratamentos atÃ© a camanda gold (Aqui representado pelo Postgres).